
"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk."
*John von Neumann*

# 0. Introduction

This is a program based on convolutional neural networks for spike detection from calcium traces, written by @PTRRupprecht and @unidesigner, as part of the Spikefinder coding challenge 2017 (http://spikefinder.codeneuro.org/). It is written in Python/Keras, with some minor (not necessarily required) parts in Matlab.

Two networks have been tested: 1) A simple three-layer CNN. 2) The same CNN, but trained with selection of the training sets that is weighted according to their similarity to the test dataset with respect to statistical properties like kurtosis, autocorrelation times, hurst coefficients etc.

1. Typical results
2. Structure of the three-layer CNN (Elephant)
3. Running the Elephant code
4. The idea behind the embedding spaces
5. Code organization related to the embedding space


# 1. Typical results

At the bottom, the recorded calcium trace is shown; in the middle, the spikes (ground truth recording). Above are predictions from the Statistics-Embedded CNN (black), the simple Elephant CNN (green) and the predictions of a less elaborate, model-based algorithm (https://github.com/PTRRupprecht/SpikefinderCompetition2017, red).

The first example is from an OGB recording.

![alt text](https://github.com/PTRRupprecht/Spikefinder-Elephant/blob/master/figures/pic1-4.png)

The second example is from a jRGECO recording that is particularly nice and easy to analyze.

![alt text](https://github.com/PTRRupprecht/Spikefinder-Elephant/blob/master/figures/pic10-7.png)

The thrid example is from a GCaMP6s recording that is particularly difficult to analyze (but this is one of the nicer cells in terms of SNR), also because the firing rate is high compared to the other datasets.

![alt text](https://github.com/PTRRupprecht/Spikefinder-Elephant/blob/master/figures/pic5-8.png)



# 2. Spikefinder-Elephant, structure of the CNN

![alt text](https://github.com/PTRRupprecht/Spikefinder-Elephant/blob/master/figures/Figure4.png)

Windowsize of the input is 128 datapoints, corresponding to 1.28 sec. Filter sizes of the convolutional filters are 41, 21 and 7 pixel for conv1d_1, conv1d_2 and conv1d_3, respectively. No zero padding was used.


# 3. Running the Elephant CNN code

0. Install tensorflow and keras into a Python virtualenv

1. Download train and test data into folders relative to this root folder:
    spikefinder.test/
    spikefinder.train/

2. Start ipython, spyder, ...

3. If needed, configure training in elephant/config_elephant.py

4. Run 'run_elephant.py'


# 4. The idea behind the embedding spaces

The ten available datasets are different in terms of calcium indicator, signal-to-noise, neuron type and brain area and possibly temperature of the recording, resulting in different optimal convolutional filters for each dataset. We try to solve this problem by first answering the following questions: Which neurons/datasets have similar convolutional filters?

To answer this question, we fit a simple 2-layer convolutional network to a single neuron and then use this network to predict the spikes for all other neurons. In the figure (left side), row 55 e.g. shows how well spikes of neuron 55 can be predicted by the networks generated by all other neurons; column 55 shows how well the model generated via neuron 55 can predict spikes of other neurons.
Normalizing over columns, symmetrizing the matrix and averaging over datasets yields the distance matrix (middle, datasets indicated with numbers).
A PCA of the distance matrix yields an embedding space, of which the first two components are plotted (right). Datasets being close to each other (e.g. 7 and 10) can predict each other's very well, whereas datasets distant from each other in space (e.g. datasets 5 and 4) are not good at predicting each other's mutual spikes.

![alt text](https://github.com/PTRRupprecht/Spikefinder-Elephant/blob/master/figures/Figure1.png)

The next challenge is to map a neuron of a dataset of unknown properties onto the right location of this embedding space. To this end, we calculated statistical properties of the raw calcium time traces: Variance, kurtosis, skewness; autocorrelation value after 0.5, 1 and 2 seconds; generalized Hurst exponents 1-5; and the power spectral density at different frequencies between 0.1 and 3.6 Hz (left). After averaging the values over datasets (middle), we used the two first principal components to generate a map of proximity in statistical property space (right). This map was generated using the training dataets (numbers on the right side of the symbols), and test datasets were mapped into the PCA space (numbers below the symbols).

![alt text](https://github.com/PTRRupprecht/Spikefinder-Elephant/blob/master/figures/Figure2.png)

Next we trained a regressor to map the statistical poperties' embedding space to the predictive embedding space. For this we used a DecisionTreeRegressor from the scikit-learn package.

![alt text](https://github.com/PTRRupprecht/Spikefinder-Elephant/blob/master/figures/Figure3.png)

In total, this procedure spans an embedding space that allows to understand the mutual predictive power of different datasets, and the mutual distance between datasets in terms of their statistical properties.

It would probably require a larger collection of diverse datasets to make this embedding robust and good for any new datasets. In the 10 datasets given, there are some outliers (e.g. dataset 5), and if a new dataset is an outlier as well (which you cannot know beforehand easily), it will not be predicted well by any model that uses those 10 datasets in a selective or attentive manner.


# 5. Code organization related to the embedding space

Execute 'run_statEmbedding.py' step by step. The procedure consists of several steps (fitting single-neuron models, generating embedding spaces, mapping between the spaces; loading data, defining the data, focused retraining of an existing model; evaluation), but the first three steps can be skipped, because they generate files that are already provided in the repository.
